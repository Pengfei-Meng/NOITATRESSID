%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                            CHAPTER FOUR                        %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 
\chapter{CUTER TEST PROBLEMS}

In this chapter, we test the Homotopy RSNK optimization algorithm and the preconditioners 
on two constructed analytical problems to investigate the algorithm's non-convexity 
handling capacity and scalability performance. The first one is a constructed indefinite 
quadratic problem with bound constraints. The second one is a constructed convex 
quadratic problem with linear inequality constraints. The second problem is scalable with 
the problem dimension ranges from 100 to 500. A state-of-the-art optimization package 
SNOPT is benchmarked with on accuracy and scalability performance.   

\section{General Information for Test Problems}
\subsection{Optimization Environment - Kona}\label{sec:kona_mv}
The previously introduced Homotopy RSNK method and the preconditioners are part of 
our in-house optimization package, Kona~\cite{dener:scitech2016}. Kona is a matrix-free, 
parallel agnostic optimization package aiming for solving large-scale PDE-constrained 
optimization problems.  It has implemented several optimization 
algorithms including an unconstrained 
reduced-space quasi-Newton method, an unconstrained reduced-space 
Newton-CG method, an equality constrained reduced-space Newton-Krylov 
based on FLECS~\cite{hicken:flecs2014}, and a composite-step RSNK.    
It contains various types of tools for supporting the functions of the optimization algorithms, e.g. 
iterative matrix-free solvers like FGMRES, STCG, FLECS for the linear systems, 
globalization techniques including trust-region, backtracking line search and line search with 
the strong Wolfe conditions, and merit functions like augmented Lagrangian and $l_2$ 
merit function. Besides, it computes the matrix-vector product of the Hessian of the Lagrangian 
by solving two second-order adjoint systems. 

Architechturally, it separates the optimization algorithms with the optimization 
problem interface, allowing the development of new optimization algorithms independently with 
new problem solvers. From a user's perspective, one has to write a solver interface to Kona providing 
a series of function operations as listed in Table 4.1 in~\cite{dener_thesis_2017}. The solver interface 
functions the same as the objective/constraint function and the sensitivity function in SNOPT, or 
other popular optimization methods like fmincon in Matlab. During the optimization process, 
conventional optimizers will call those functions for evaluating the function values and sensitivities 
values at different design points, then make decisions on where to go for the next point. 
The sensitivities demanded are the total gradients. In presence of state variables for 
PDE-constrained optimization, total gradients are expensive as each one would require an adjoint solve, 
as explained in detail in Chapter~\ref{sec:pde_mot}. In contrast, the structure of the solver interface 
required by Kona is designed specially for PDE-constrained optimization problems by providing 
partial gradients instead of total gradients, and matrix-vector products with the system matrix of 
the PDE solvers, with the Jacobians of the constraints w.r.t. the state variables, and with 
the Jacobians of the constraints w.r.t. the design variables. These matrix-vector products 
with the Jacobians are used to assemble the matrix-vector products with the Lagrangian Hessian 
and the total constraint gradients, which in turn are used by the Krylov solvers to solve the linear systems.    

The Homotopy RSNK method developed in this work is placed together with the other unconstrained 
and equality constrained optimization methods, with the same interface and using the same 
vector and matrix operator modules. The preconditioners are likewise placed in the preconditioned folders. 
When testing different problems, the user will first write the solver interface, then setup the running script 
to execute the optimization. 

Although Homotopy RSNK's strength is in PDE-constrained optimization area in presence of 
state variables, it can still be used on problems without state variables, where the conventional 
optimization methods dominate.  For such problems, the total gradients are usually cheap to 
calculate and readily available. For instance, to solve a linear system involving the total constraint Jacobian, 
conventional optimization methods would ask to compute and 
store these total gradients for once, then find the solution by operating directly to the matrix entries.  
In contrast, when using the Homotopy RSNK method, it would ask to calculate the matrix-vector 
products several times in order to form the Krylov subspace. Each time the constraint Jacobian is 
calculated and multiplied with the incoming vector, then released from the memory space. 
So it's the memory cost of the storage of the matrix versus the computational cost of matrix-vector products 
for several times. For some problems, the matrix exists in the form of basic algebraic operations, saving the memory cost of storing the matrix even for once. 


\subsection{SNOPT}
SNOPT is short for Sparse Nonlinear OPTimizer~\cite{gill:2002}, a gradient-based sequential 
quadratic optimization method for solving large-scale nonlinear optimization problem 
with thousands of constraints and design variables. It uses an augmented Lagrangian 
merit function, and the Hessian of the Lagrangian is approximated
using a limited-memory quasi-Newton method. In this work, we use its Python interface 
pyOpt to compare its performance with that of the proposed Homotopy RSNK method. 
SNOPT uses major feasibility and major optimality to measure the progress of the 
optimization iteration. The feasibility measures the maximum nonlinear constraint violation, 
normalized by the size of the solution,
\begin{equation*}
\text{snopt feasibility} = \underset{i}{\text{max}}  \ \text{viol}_i / \lVert x \rVert 
\end{equation*}
where $\text{viol}_i$ is the violation of the $i$th nonlinear constraint~\cite{snopt_manual}. The feasibility tolerance 
$\epsilon_r$ is predefined, with a default value of $10^{-6}$. If it is known that some of the problem 
functions are of low accuracy, a larger value for the feasibility tolerance is appropriate. 

The optimality measures the maximum complementarity slackness for the design variables, 
and is calculated using the following formulas, 
\begin{equation*}
\text{snopt optimality} = \underset{j}{\text{max}}  \ \text{Comp}_j / \lVert \pi \rVert 
\end{equation*}
where $\text{Comp}_j$ measures the complementarity slackness for $j$th variable and is defined by:
\begin{equation*}
\text{Comp}_j = \begin{cases}
d_j \text{min} (x_j - l_j, 1)  \quad \text{if} \ d_j \geq 0 \\
-d_j \text{min} (u_j - x_j, 1) \quad \text{if} \ d_j < 0
\end{cases}
\end{equation*}
where $d_j = g_j - \pi^T a_j$ is the gradient of the Lagrangian, $g_j$ is the $j$th objective gradient, 
$a_j$ is the $j$th constraint Jacobian, $\pi$ is the dual variables. 

\subsection{Basic Settings for the Algorithm}
To measure the progress of the optimization iterations, we evaluate the infinite
norm of each row block in $F(x)$. Specifically, we will refer to optimality,
complementarity, and feasibility as defined below:
\begin{align*}\label{eq:optfeas}
\text{Optimality} &= \lVert   \nabla_x f(x) + \lambda^T \nabla_x g(x)  \rVert _{\infty} \\
\text{Complementarity} &=  \lVert   -\mat{S}\mat{\Lambda} e   \rVert _{\infty}   \\
\text{Feasibility} &= \begin{Vmatrix} h(x) \\ g(x) - s  \end{Vmatrix} _{\infty} 
\end{align*}

In the following tests, the convergence plots display the above metrics versus
computational cost.  Note that the absolute values of the above metrics are used, 
as the initial complementarity products are zero because of zero initial multipliers, 
and the initial feasibility is zero when we use $s_0 = g(x_0)$ for the inequality-only 
structural problem. 

For simplicity, the convergence plots show only the
predictor points before $\mu$ reaches $\epsilon_\mu$, plus all the corrector
points at the last homotopy iteration when $\mu < \epsilon_{\mu}$.

Table \ref{tab:param} shows the parameters used in the test problems, including
the default values set in the algorithm and the recommended ranges.

\begin{table}[tbp]
  \begin{center}
    \caption{Parameters used in the test problems \label{tab:param}}
  \begin{tabular}{ l c c c c c}
    \textbf{Parameters} & $\textbf{Default}$  & $\textbf{Range}$ & $\textbf{Nonconvex}$ 
    & $ \textbf{Quadratic} $ & \textbf{Structural}  \\ \hline
    %\rule{0ex}{3ex}%
    \multicolumn{6}{ l }{Predictor-Corrector Algorithm} \\   %  $\delta_{\text{targ}}$ and $\phi_{\text{targ}}$. 
    \hline
    $K_{\max}$             	&  100     & $\geq$100         & 100 	 &  100       &    100     \\ 
     $J_{\max}$  		&   2         & $\geq$2         & 2             & 2           &      2        \\
    $\epsilon_F$ 	           		&  1e-6     & [1e-8, 1e-3]   & 1e-7 	 & 1e-7       &    1e-4    \\ 
       $\tau$    		&   0.1      & [0.1,0.5]	        & 0.1          & 0.1	 &     0.1    \\
    $\epsilon_H$    		&   0.1      & [0.1,0.5]	        & 0.1          & 0.1	 &     0.1    \\
    $\epsilon_{\mu} $   & 1e-9     & [1e-10, 1e-6]  &   1e-9  & 1e-9  & 1e-6  \\   
    \textbf{$\alpha_0$}             &  0.05     & $\geq$0.01         & 0.05	 & [40,60,80,100,120]  &  0.05  \\
    $\delta_{\text{targ}}$      &  1.0	& [1.0,10]          & 10		 & 10    &   1.0    \\
    $\phi^{\circ}_{\text{targ}}$   & 10.0	& [5.0,50] 	       & 10		 & 20    &   10     \\
    $\zeta_{\max}$ 		        &  50		& [10,50]	       & 50		 & 50    	 &   50     \\
    $\zeta_{\min}$ 		        &  0.5	& 0.5		       & 0.001	 & 0.001    &   0.5   \\
    $\Delta \mu_{\max}$		        &  -5e-4	& [-5e-4, -5e-1]  & -5e-4	 & -5e-4     &  -5e-4  \\  
    $\Delta \mu_{\min}$		        &  -0.9	& -0.9 	       & -0.9		 & -0.9       &  -0.9    \\
    $s_0$                           & $\mathbf{e}$     &   $>$ 0    &    5$\mathbf{e}$    &  10$\mathbf{e}$   &  $g(x_0)$  \\ 
    $\tau_s$                      & 1e-6    & 1e-6    &  1e-6    &  1e-6    & 1e-6   \\
    \hline
    \multicolumn{6}{ l }{Preconditioner} \\ 
    \hline    
    $\mathbf{{n_{\mat{\Sigma}}}}$    & 5	       & $\geq$2		&  -	         &  2          &  [20,80,320]  \\
    $\beta$				& 1.0	       & $>$0        & -         &  -      &  0.1  \\
    $N_{\text{bfgs}}$		& 10	       & [1, 20]		& - 		 &  10	& -  \\
    $\mu_e$			& -1	       & [0, 1] 		& -	         &  -	        & 1e-3  \\
    $\Sigma_e$			& 1 	       & [0, 1]                 & -		& -		& 1e-3  \\
    \hline
    \multicolumn{6}{ l }{Krylov Iterative Solver} \\ 
    \hline       
    $n_k$		& 20        & [10,30]              & 20		 &  20       &  20  \\
    $\epsilon_{\text{krylov}}$		& 1e-2     & [0, 0.1]           	&1e-2	 &  1e-2    &  1e-4  \\
    \hline
  \end{tabular}
  \end{center}
\end{table}

\section{CUTEr Problem Description}\label{sec:cuter1}
The \textbf{CUTEr} Test Problem Set~\cite{cuter_opt, cuter_gould} is a huge collection of test problems to test new optimization codes and develop new algorithms. The problem set ranges from small differentiable unconstrained problem to large scale dense and sparse problems with both equality and inequality constraints, nonlinear systems, and network problems etc.  Some of the test problems exhibit numerical difficulties observable in practice like bad scaling in objective and constraint functions, ill-conditioned problems, multiple local solutions, non-regular solutions where the constraint qualification is not satisfied etc. A number of optimization packages have interfaced with CUTEr~\cite{cuter_interface} like Ipopt, Knitro, Minos, SNOPT etc.  

The problems are written in Standard Input Format (SIF)~\cite{Conn1992}. A decoder works to translate the problems written in SIF to data files in Fortran77, which would provide tools to access the function values, Jacobians, and sometimes Hessians to the optimization packages. It is important for a developing optimization algorithm to test on a good, quality, and suitable subset of problems in CUTEr/CUTEst to validate its accuracy and robustness. 

\subsection{A quick introduction to CUTEr test problems}\label{sec:cuternm}
CUTEr provides access to the objective function and the constraint function, together with the gradients for the test problems. The objective value is a real scalar: 
\begin{equation*}
y = f(x) 
\end{equation*}
where  $x \in \mathbb{R}^n, y \in \mathbb{R}$. $x_i$ is the i-th component of x. 

The constraints include bound constraints on the variable and other types of constraints. The variable $x$ are subject to simple bounds:
\begin{equation*}
\text{bl}_i \leq x_i \leq \text{bu}_i  
\end{equation*}
where $\text{bl}_i$ and $\text{bu}_i$ are the lower and upper bound on $x_i$. When there is no lower or upper bound on $x_i$, then $\text{bl}_i = -1e20$ or $\text{bu}_i = 1e20$. 

Besides bound constraints, all the other constraints are gathered in a single vector-valued function $c(x) \in \mathbb{R}^m$, and constrained in the form:    
\begin{equation*}
\text{cl}_i \leq c_i(x) \leq \text{cu}_i  
\end{equation*}
where either $\text{cl}_i$ or $\text{cu}_i$ is always infinite. This means that the inequality constraint must take just one of the form below for a certain problem:
\begin{equation*}
c_i(x) \geq 0   \quad  \text{or}  \quad  c_i(x) \leq 0
\end{equation*}

For equality constraints $\text{cl}_i$ and $\text{cu}_i$ are both equal to 0. There is also a bool vector $\text{Equatn} \in \mathbb{R}^m$ indicating whether a constraint is an equality constraint or not. 

It is possible to order CUTEr to place equality constraints before inequality constraints, or linear constraints before nonlinear constraints by reordering the components of $c(x)$. Likewise, components of $x$ can be reordered such that nonlinear variables appear before linear ones. 

In addition, CUTEr also outputs the Lagrangian function value, the gradient of the objective function and the Lagrangian, the Jacobian matrix and the Hessian matrix of the constraints. 

\subsection{Kona-CUTEr Interface}
By using a python interface to CUTEr~\cite{cuter_python}, one can access the test problems in Python environment and build the Kona-CUTEr interface. As described in~\cite{dener:scitech2016}, a Kona solver interface essentially asks for matrix-vector products with the system matrix for PDE solvers, and matrix-vector products with constraint Jacobians for equality and inequality constraints separately. As CUTEr test problems do not possess state vectors, and not governed by physics system, the matrix-vector products with the system matrix for PDE solvers are not needed. For the constraint Jacobian products, the explicit Jacobian matrices from CUTEr can be readily used to calculate their products with an arbitrary incoming vector. 

As discussed in chapter \ref{sec:kona_mv}, Kona might not be able to solve CUTEr problems as fast as conventional optimization methods like SNOPT, because the total Jacobian matrix are readily available in CUTEr problems and there is no state variables.  Nonetheless, CUTEr problems are still valuable to validate Kona's accuracy, and capability to overcome other numerical difficulties, and to handle non-convex problems. 

\subsection{Problem Classification}
Each problem in CUTEr is classified following the Hock and Schittkowski scheme~\cite{cuterScheme} by the string: 
\begin{equation*}
X \ X \ X \ r \ - \ X \ X \ - \ n \ - \ m
\end{equation*}

The first character defines the problem objective function type, with the following options: 
\textbf{U}: undefined, \textbf{C}: constant, \textbf{L}: linear, \textbf{Q}: quadratic, \textbf{S}: sum of squares, \textbf{O}: none of the above. 

The second character defines the constraint function type, with the options: 
\textbf{U}: unconstrained, \textbf{X}: fixed variables, \textbf{B}: bounds on the variables, \textbf{N}: adjacency matrix of a linear network, \textbf{L}: linear, \textbf{Q}: quadratic, \textbf{O}: more general constraints. 

The third character shows the smoothness of the problem, with the option of \textbf{R} indicating the problem is regular, and its first and second derivatives exist and continuous everywhere; and \textbf{I}: the problem is irregular. The third character \textbf{r} holds an integer among 0, 1 and 2, indicating the highest derivatives provided analytically.

The first character after the hyphen indicates the origin of the problem, with the option of \textbf{A}: the problem is academic, mainly used by researchers to test algorithms; \textbf{M}: the problem is a modeling exercise, with the solution not used in practical application; \textbf{R}: the solution of the problem has been used in real application.  

The next character shows whether the problem contains internal variables, with the option of \textbf{Y} and \textbf{N}. 

The last two characters \textbf{n} and \textbf{m} indicate the number of variables and constraints (fixed variables and bound constraints excluded) in the problem respectively, with the option of \textbf{V}: an integer chosen by the user, or a constant integer giving the fixed numbers. 

\section{Results}
As the CUTEr test problem set contains a huge collection of problems with assorted features, due to limited time and resources, only a subset of the problems are considered here. Users can select subsets of the problems that belong to a certain category as described in Section~\ref{sec:cuter1} by using Python CUTEr interface. Problems with the following features are selected:
\begin{equation*}
Q \ L \ R \ 2 \ - \ A \ X \ - \ [1,500] \ - \ [1,500]
\end{equation*}
that is, problems with quadratic objective function (convex, non convex, or indefinite), linear constraint function, with second order derivatives and continuous everywhere, from academic area, used by researchers to test algorithms, finally the number of design variables in $[1,500]$ and the number of constraints in $[1,500]$.  Table~\ref{tab:cuter} lists the results on the selected subset of the Cuter problems. The first column 'Name'  are the name of the problems as in \cite{cuter_probs}, where the ascii files are also available that contains information on the problem including problem origin, authors, classifications, SIF problem cards, and sometimes the solutions. The second column 'n, m' are the number of design variable $x$ and constraints as described in~\ref{sec:cuternm}. The third column 'n,  $m_{\text{eq}}$,  $m_{\text{ineq}}$' are the number of $x$, equality constraints and inequality constraints interpreted in Kona's way. The fourth column describes the key word for the origin or origin of the problem. The fifth column lists the major parameters used in the Homotopy RSNK algorithm, the initial step size $\textbf{$\alpha_0$}$, the nominal distance  $\delta_{\text{targ}}$  and nominal angle $\phi^{\circ}_{\text{targ}}$ as defined in Section~\ref{sec:step}, and the rank of the SVD approximation in Lanczos method used in the preconditioner~\ref{eq:svd}. The sixth column is the optimized objective function value using Kona, while the seventh column is that using SNOPT. The last column is the optimized objective function value as shown in the ascii file of~\cite{cuter_probs}. Note that some problems do not provide solutions, thus 'N/A' is used. 

\begin{landscape}
\begin{longtable}{l | l |  l  |  >{\footnotesize}p{3.5cm} | l | c | c | c  }      %\toprule      % |  >{\centering}m{3.5cm}
\caption{Subsets of Cuter Problem Results}\label{tab:cuter} \\
 \hline 
Name   &                 n,   m       &  n,  $m_{\text{eq}}$,  $m_{\text{ineq}}$        &     Origin      &\textbf{$\alpha_0$},  $\delta_{\text{targ}}$, $\phi^{\circ}_{\text{targ}}$,  $\mathbf{{n_{\mat{\Sigma}}}}$      & $ f_{\text{kona}} $   & $ f_{\text{snopt}} $ &$ f^*$    \\ \hline
AUG2D    &           24,  9         &  24,   9,  18       &    2-D Laplace   & 0.05,1,5,30   &  0.124999           &     0.1250    &      N/A                \\ \hline
AUG2D    &          220, 100   & 220, 100, 200  &                      &   0.05,1,5,30         &   110.7987          &    110.7991      &    N/A           \\ \hline
AUG2DC  &       24, 9      &   24, 9, 18       &       &     0.05,1,5,30        &    2.973213        & 2.973214    &   N/A     \\ \hline
AUG2DC  &      220, 100   &  220, 100, 200   &     &   0.05,1,5,30    & 184.2388    &   184.2394     &    N/A      \\ \hline
AUG3D  &      156, 27    & 156, 27, 54   & 3-D Laplace  &  0.05,1,5,30   & 0.08333   &   0.083333   &  N/A         \\ \hline
AUG3DC  &  156, 27  &  156, 27, 54   &    & 0.05,1,5,30 &   35.84226   &   35.84276      & N/A      \\ \hline
AVGASA  &       8, 10   &   8, 0, 26   &            LP problem     &   0.05,1,5,30       &           -4.63092    &    -4.79278   &   N/A     \\ \hline
AVGASB  &    8, 10  &   8, 0, 26   &   LP problem      & 0.05,1,5,30   &   -4.482206   &   -4.666351     &   N/A    \\ \hline
ALLINQP  & 10, 5   &  10, 1, 21   & banded QP  &  0.05,10,20,30 & 0.346667  & -0.183256     &  N/A \\  \hline     %\midrule
BLOCKQP2   & 25, 11   & 25,  10, 71   &    non-convex  & 0.05,1,5,5  & -6.201652   & -2.507e+14    &  -6.2017   \\ \hline
BLOCKQP3   & 25, 11   & 25,  10, 71  &  	  & 0.05,1,5,2 & 2.330508     &  2.330508   &            -2.4987e-1   \\ \hline
BLOCKQP4  & 25, 11    & 25, 10, 71  &    & 0.05,1,5,5     & -2.928928   & -5.73e+14   &    -2.499e-1   \\ \hline
BDRY2   & 25, 18   & 25, 18, 86   &  AMPL  & 0.05,1,5,3  & 0.544932  & 0.548110  & N/A    \\ \hline   
BIGGSC4  & 4, 7   & 4, 0, 21   & & 0.05,1,5,2  & -24.4999   &  -24.375  &    -24.5 \\ \hline 
CVXQP1  & 10, 5 & 10, 5, 30  & convex   & 0.05,1,5,2 & 181.0401 & 165.8738  & N/A   \\ \hline
DEGENQP & 10, 1005 & 10,5,2030  & degenerate convex  & 0.05,1,5,10  & 8.80e-06 & -5.55e-17  & N/A \\ \hline
DTOC3   & 29, 18  & 29,18,40 & discrete time control & 0.05,1,5,10  &  224.5904 & 8.20189  & 224.5904 \\ \hline 
GENHS28 & 10, 8  & 10, 8, 16  & Hock and Schittkowski &   0.05,1,5,2   & 0.927173  & 0.927174  & 0.0   \\ \hline
GMNCASE1 &  175, 300   & 175, 0, 300  &   optimized control   & 0.5, 5, 5, 100   &   0.267087  & 0.266973             &    0.266733   \\ \hline
GMNCASE4 & 175, 350 & 175, 0, 350  &  optimized control &  0.5, 5, 5, 100      & 5.6273e3 &  5.9469e3 &    5.9468e3 \\ \hline 
HS268   & 5,5  & 5, 0, 5  &  Schittkowski   &  0.05,50,5,3    & 2.09e-4   & -3.6e-12      &    N/A    \\ \hline 
HS21    &  2,1 & 2,0,5   & Hock and  Schittkowski  &  0.5,5,10,1   & -99.3434  & -99.9900  & -99.96    \\ \hline 
HS35   &  3,1  & 3,0,4  &             &  0.05,1,5,1    & 0.111111  & 0.111111   &   0.111111   \\ \hline 
HS35I   &  3,1  & 3,0,7  &             &   0.05,1,5,1      & 0.111111  & 0.111111   &   0.111111   \\ \hline 
HS44   & 4,6   & 4,0,10   &  	 & 0.05,1,20,3      &  -13.000 & -4.01e+14 & -13.0    \\ \hline
HS44NEW & 4,6  & 4,0,10  &     &   0.05,1,20,3    & -13     & -3.20e+14   & -13.0    \\ \hline
HS51    &  5,3   & 5,3,6    &  & 0.05,1,20,3      & 4.46e-19   & 5.86E-14     & 0.0    \\ \hline
HS52    &  5,3  & 5,3,6   & & 0.05,1,20,3      & 5.326634   & 5.326647   & 5.326643   \\ \hline
HS53    &  5,3  & 5,3,16    &   &  0.05,1,5,3  & 4.093023   & 4.093023   & 4.093023  \\ \hline
HS118  & 15,17  & 15,0,59  & & 0.05,50,5,3   & 664.8205  & -1748.638  & 664.8204    \\ \hline
HS268  & 5,5  & 5,0,5  & & 0.05,50,5,3     & 3.839e-4 & -3E-12  & N/A   \\ \hline
HATFLDH  & 4, 7 &  4, 0, 21  & & 0.05,1,5,2  &  -24.5002  & -24.375 & 24.5   \\ \hline
LOTSCHD  & 12,7  &  12, 7, 26 & eco. lot scheduling &  0.05,1,5,4   & 44.2890 & 165.6553  & N/A  \\ \hline
MOSARQP1 & 36, 10 & 36, 0, 46 &convex quadratic & 0.05,1,5,4     &  -24.14365 & -52.04917 & -24.13768 \\ \hline
MOSARQP2 &36, 10 & 36, 0, 46 &     & 0.05,1,5,4   & -35.69815  & -55.16234  & -35.6981 \\ \hline
POWELL20 & 10,10  & 10,0,10 & degenerate convex & 0.05,1,5,2    & 57.8125 & 57.8125  & N/A  \\\hline
RDW2D52F & 18,1  & 18,1,36 & optimal control &  0.05,10,20,5 &  0.053016 & 0.020779 & N/A \\\hline
STCQP1  & 17, 8  &  17, 8, 50 & convex   &    0.05,1,5,5  & 494.4054  & 494.5208  & 4.95E+02 \\\hline
SOSQP1 &  20, 11 & 20, 11, 62 & non-convex & 0.05,1,5,2    & 5.9e-07 & -4E-16   & 0.0 \\\hline
SOSQP2 & 20,11  &  20, 11, 62 &    & 0.05,1,5,5	& -3.99779  & -4.04565  & -3.99781    \\\hline
S268   &   5,5   & 5,0,5  &   & 0.05,10,20,10  &   2.274e-3  &  -3.64E-12  & N/A  \\\hline
YAO   &  22,20  & 22, 0, 25 &   &  0.05,1,5,10 & 2.398829  & 3.715e-3 & 2.39883  \\\hline
ZECEVIC2 & 2,2 & 2,0,6  & &  0.05,1,5,2 & -4.1249  & -4.125  & N/A  \\\hline
\end{longtable}   % \midrule
\end{landscape}


As can be seen, for most problems, the proposed Homotopy RSNK method and the preconditioner can deliver accurate solutions.  However, even with a small number of numerical tests, several valuable insights could be drawn: 
\begin{enumerate}
\item While the current Homotopy RSNK optimization algorithm works as a general optimization method, irrelevant of the problem types and feature, it struggles on problems with any hint of ill-conditioning, such as degenerate problems where the constraint qualifications are not satisfied, badly scaled objective and constraint functions. 
\item As CUTEr test problems do not involve the state variables and thus adjoint variables, the Homotopy RSNK's core strength of using second-order adjoints to approximate Hessian vector products is not applicable. In contrast, SNOPT can readily make good use of the explicit Jacobians 
provided by the pyCUTEr tool, while Kona is only using the explicit matrices to calculate the products in order to form the Krylov subspace. Consequently, SNOPT is faster than Kona when solving the CUTEr problems on average.   
\item The proposed matrix-free preconditioner is not general and is specifically designed for PDE-constrained optimization problems as introduced at the beginning of this thesis.  For bound-only constraints, it does not work that well. Because the constraint Jacobians are Identity matrix, the Krylov subspace built from it is only 1-rank, making the Lanczos method's SVD approximation very insufficient.  
\item If the product of the constraint Jacobian with a vector of ones is always zero, then it will make the preconditioner crash. Because the Krylov subspace would be linearly dependent, making the Lanczos methods fail. Problems whose constraint Jacobian has the non-zero entries of e.g. $[-1, -3, 3, 1]$ on each row belong to this category, like the LISWET problems 
\item For non-convex problems, sometimes the proposed Homotopy RSNK can work, and sometimes not. So more effort is needed to increase the robustness of the algorithm. 
\end{enumerate} 



